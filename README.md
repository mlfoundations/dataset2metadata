# dataset2metadata
[![pypi](https://img.shields.io/pypi/v/dataset2metadata.svg)](https://pypi.python.org/pypi/dataset2metadata)

Process a downloaded dataset (currently a [webdataset](https://github.com/webdataset/webdataset) generated by [img2dataset](https://github.com/rom1504/img2dataset)) with arbitrary models. Currently the default workload supports metadata processing from the DataComp project.

## default.yml
Here we provide a default `yml`, which does preprocessing equivilant to that used in the DataComp project.
This includes computing:
* OpenAI [CLIP ViT-B/32](https://github.com/openai/CLIP) image, text features and CLIP scores
* OpenAI [CLIP ViT-L/14](https://github.com/openai/CLIP) image, text features and CLIP scores
* [detoxify](https://github.com/unitaryai/detoxify) text toxicity scores
* NSFW image filtering (custom trained classifier on CLIP ViT-L/14 images featues)
* [ISC Descriptor](https://github.com/lyakaap/ISC21-Descriptor-Track-1st) features and near-duplicate scores against the DataComp evaluation sets.

```yaml
models: # model directives, specifying the models to instantiate
  - oai-clip-vit-b32
  - oai-clip-vit-l14
  - nsfw-detoxify
  - nsfw-image-oai-clip-vit-l-14
  - faces-scrfd10g
  - dedup-isc-ft-v107
postprocess_columns: # postprocessing directives
  - oai-clip-vit-b32-score
  - oai-clip-vit-l14-score
  - nsfw-detoxify-score
  - nsfw-image-score
  - face-boxes
  - dedup-isc-ft-v107-score
postprocess_features: # saved in an npz format
  - oai-clip-vit-b32-image
  - oai-clip-vit-b32-text
  - oai-clip-vit-l14-image
  - oai-clip-vit-l14-text
  - dedup-isc-ft-v107-image
additional_fields: # fields in a webdataset json to carry over into the metadata
  - uid
  - url
  - caption
  - original_width
  - original_height
  - sha256
nworkers: 2
batch_size: 512
device: 0
input_tars: "path/to/my/tars/000057{17..19}.tar" # braceexpand suported, can also be s3 paths
output_metadata_dir: "path/to/my/ouput/metadata" # can be arbitrary path
custom_pypath: null # if model, preprocessors, postprocessors not known, look in this python file for user provided custom implementation
```

## custom.yml
To provide more flexibility for user workloads, we additionally support users providing custom models.
Here we provide an example to compute:
* OpenAI [CLIP ViT-B/32](https://github.com/openai/CLIP) image, text features and CLIP scores (already supported)
* [BLIP2](https://huggingface.co/docs/transformers/main/model_doc/blip-2) image caption generation


```python
# custom.py implementation

from functools import partial

import torch
import torch.nn as nn
from transformers import Blip2ForConditionalGeneration, Blip2Processor

from dataset2metadata.postprocessors import identity

hf_preprocessor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")

# define model here
class Blip2Wrapper(nn.Module):

    name = 'blip2' # static field: provide name of model
    raw_inputs = ['image', ] # static field: provide name of input to process
    preprocessors = ['blip2-aug', ] # static field: name of preprocessor
    dependencies = [] # static field: other models that should be evaluated before this model
    to_device = True # static field: if True, move input to device

    def __init__(self, device) -> None:
        super().__init__()
        self.model = Blip2ForConditionalGeneration.from_pretrained(
            "Salesforce/blip2-opt-2.7b",
            torch_dtype=torch.float16).to(device)

        self.model.eval()
        print(f'instantiated {self.name} on {device}')

    def forward(self, x):
        generated_ids = self.model.generate(pixel_values=x)
        generated_text = [
            t.strip() for t in bp.batch_decode(generated_ids, skip_special_tokens=True)
        ]

        return generated_text

def blip_preprocess(x):
    # custom preprocessor function

    if x.height == 1 and x.width == 1:
        # edge case as huggingface tries to guess the channel dim
        x = x.resize((2, 2))

    if x.height == 3 and x.width == 3:
        # edge case as huggingface tries to guess the channel dim
        x = x.resize((4, 4))

    a = hf_preprocessor(images=x, return_tensors="pt").to(torch.float16)

    return a['pixel_values'].squeeze()

# map preprocessor strings to preprocessor functions
preprocessor_lookup = {
    'blip2-aug': blip_preprocess,
}

# map model strings to model classes
model_lookup = {
    'blip2': Blip2Wrapper,
}

# map postprocessor strings to functions, outputs saved to column-store parquet
postprocess_parquet_lookup = {
    'blip2-cap': partial(identity, model='blip2', to_cpu=False),
}
# map postprocessor strings to functions, outputs saved to feature store npz file
postprocess_feature_lookup = {}
```

The corresponding `yml` looks as follows
```yml
# custom.yml

models: # model directives, specifying the models to instantiate
  - oai-clip-vit-b32
  - blip2
postprocess_columns: # postprocessing directives
  - oai-clip-vit-b32-score
  - blip2-cap
postprocess_features: # saved in an npz format
  - oai-clip-vit-b32-image
  - oai-clip-vit-b32-text
additional_fields: # fields in a webdataset json to carry over into the metadata
  - uid
  - url
  - caption
  - original_width
  - original_height
  - sha256
nworkers: 2
batch_size: 512
device: 0
input_tars: "path/to/my/tars/000057{17..19}.tar" # braceexpand suported, can also be s3 paths
output_metadata_dir: "path/to/my/ouput/metadata" # can be arbitrary path
custom_pypath: "path/to/my/custom.py" # if model not known, look in this python file for implementation
```

## Command lines

To run:

```bash
dataset2metadata --yml_path=example.yml
```